{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751a9b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allmäna imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "\n",
    "# Scikit-learn \n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold # Added RandomizedSearchCV and StratifiedKFold\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, PowerTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# För evaluering\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, RocCurveDisplay\n",
    "from scipy.stats import uniform, randint # Needed for RandomizedSearchCV distributions\n",
    "\n",
    "df = pd.read_parquet('train_delay_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6360e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime_cols = ['TripStartDate']\n",
    "for col in datetime_cols:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_datetime(df[col])\n",
    "df['DayOfWeek'] = df['TripStartDate'].dt.dayofweek\n",
    "\n",
    "df['PlanedDuration'] = (df['ArrivalAdvertised'] - df['DepartureAdvertised']).dt.total_seconds() / 60.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81251639",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_feature_groups(df):\n",
    "    \"\"\"Defines which columns belong to which processing group.\"\"\"\n",
    "    \n",
    "    # Numerical features that are right-skewed and need transformation\n",
    "    numerical_features = ['DistanceKm', 'PlanedDuration']\n",
    "    \n",
    "    # Nominal categorical features (including those with NaNs)\n",
    "    nominal_categorical_features = [\n",
    "        'departure_station', 'arrival_station', 'end_station_county',\n",
    "        'TrainOwner', 'Operator', 'trip_typeoftraffic',\n",
    "        'DayOfWeek', 'start_hour' # Treat DayOfWeek and start_hour as nominal for OHE\n",
    "    ]\n",
    "\n",
    "    # Binary/Ordinal features that require minimal processing (or none)\n",
    "    binary_features = ['is_weekday'] \n",
    "    \n",
    "    return numerical_features, nominal_categorical_features, binary_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc572cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_preprocessor(numerical_features, nominal_categorical_features, binary_features):\n",
    "    \"\"\"\n",
    "    Creates a ColumnTransformer to apply different preprocessing steps\n",
    "    to different columns.\n",
    "    \"\"\"\n",
    "    # Pipeline for Numerical Features (DistanceKm, PlanedDuration)\n",
    "    # 1. PowerTransformer handles the right-skewed distribution.\n",
    "    # 2. StandardScaler ensures features have zero mean and unit variance.\n",
    "    numerical_pipeline = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', PowerTransformer(method='yeo-johnson', standardize=True))\n",
    "    ])\n",
    "\n",
    "    # Pipeline for Nominal Categorical Features\n",
    "    # 1. Imputer fills missing values (e.g., in TrainOwner/Operator/trip_typeoftraffic)\n",
    "    # 2. OneHotEncoder converts categories into binary features.\n",
    "    categorical_pipeline = Pipeline(steps=[\n",
    "        # Use 'most_frequent' for nominal data, fill_value='missing' works well for OHE\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "        # handle_unknown='ignore' ensures robustness against new categories in test set\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "    ])\n",
    "\n",
    "    # Combine all pipelines into a ColumnTransformer\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numerical_pipeline, numerical_features),\n",
    "            ('cat', categorical_pipeline, nominal_categorical_features),\n",
    "            ('bin', 'passthrough', binary_features) # No transformation needed for is_weekday\n",
    "        ],\n",
    "        remainder='drop' # Drop any other columns not specified\n",
    "    )\n",
    "    return preprocessor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6ea99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_transformed_dataframe(df, preprocessor, nominal_categorical_features, numerical_features, binary_features):\n",
    "    \"\"\"\n",
    "    Applies the preprocessor to the input DataFrame and returns a new DataFrame\n",
    "    with proper column names, ready for visualization and inspection.\n",
    "    \"\"\"\n",
    "    # 1. Fit and transform the data\n",
    "    X_transformed = preprocessor.fit_transform(df.drop(columns=['is_delayed']))\n",
    "\n",
    "    # 2. Extract feature names from the OneHotEncoder\n",
    "    # Get the 'onehot' step from the 'cat' transformer\n",
    "    ohe_step = preprocessor.named_transformers_['cat'].named_steps['onehot']\n",
    "    \n",
    "    # Get the names of the encoded columns\n",
    "    feature_names_out = list(numerical_features)\n",
    "    feature_names_out.extend(ohe_step.get_feature_names_out(nominal_categorical_features))\n",
    "    feature_names_out.extend(binary_features)\n",
    "\n",
    "    # 3. Create the final DataFrame\n",
    "    transformed_df = pd.DataFrame(X_transformed, columns=feature_names_out)\n",
    "    \n",
    "    # 4. Add the target variable back (since we dropped it earlier)\n",
    "    transformed_df['is_delayed'] = df['is_delayed'].values \n",
    "    \n",
    "    print(f\"\\n--- Transformed DataFrame Dimensions for Visualization ---\")\n",
    "    print(f\"Shape: {transformed_df.shape}\")\n",
    "    print(f\"Columns: {len(transformed_df.columns)}\")\n",
    "    \n",
    "    return transformed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3652df58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_rename_future_data(df_future):\n",
    "    \"\"\"\n",
    "    Transforms the future data schema to match the trained model's input features.\n",
    "    \"\"\"\n",
    "    print(\"\\nCleaning and renaming future data to match trained pipeline schema...\")\n",
    "    \n",
    "    # 1. Define the necessary columns (matching the original training features)\n",
    "    # The model expects these 11 columns (plus the target 'is_delayed' which is not present)\n",
    "    required_feature_map = {\n",
    "        'distance_km': 'DistanceKm',\n",
    "        'duration_minutes': 'PlanedDuration',\n",
    "        'start_weekday_': 'DayOfWeek',\n",
    "        'start_hour': 'start_hour',\n",
    "        'is_weekday': 'is_weekday',\n",
    "        'start_station': 'departure_station',\n",
    "        'end_station': 'arrival_station',\n",
    "        'ToLocations': 'end_station_county', # Assuming 'ToLocations' holds the county info\n",
    "        'TrainOwner': 'TrainOwner',\n",
    "        'Operator': 'Operator',\n",
    "        'start_typeoftraffic': 'trip_typeoftraffic'\n",
    "    }\n",
    "\n",
    "    # 2. Select and rename columns\n",
    "    df_cleaned = df_future[list(required_feature_map.keys())].rename(columns=required_feature_map)\n",
    "\n",
    "    # 3. Ensure data types match the categories used during training\n",
    "    # The preprocessor expects these to be categorical, even if they contain integers\n",
    "    categorical_cols = ['DayOfWeek', 'start_hour', 'departure_station', 'arrival_station', \n",
    "                        'end_station_county', 'TrainOwner', 'Operator', 'trip_typeoftraffic']\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        # We must cast back to category type for the fitted OneHotEncoder to work correctly\n",
    "        df_cleaned[col] = df_cleaned[col].astype('category')\n",
    "        \n",
    "    print(\"Future data schema matched successfully.\")\n",
    "    return df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d205414",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = df.copy()\n",
    "\n",
    "X = df.drop('is_delayed', axis=1)\n",
    "y = df['is_delayed']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "numerical_features, nominal_categorical_features, binary_features = define_feature_groups(df)\n",
    "\n",
    "preprocessor = build_preprocessor(numerical_features, nominal_categorical_features, binary_features)\n",
    "\n",
    "# --- Step A: Visualization of Transformed Data (Skipped plotting for brevity in this update) ---\n",
    "\n",
    "# Generate the visualization-ready DataFrame (still useful for context)\n",
    "df_transformed = get_transformed_dataframe(\n",
    "    df, \n",
    "    preprocessor, \n",
    "    nominal_categorical_features, \n",
    "    numerical_features, \n",
    "    binary_features\n",
    ")\n",
    "\n",
    "# \n",
    "print(\"\\nVisualization and initial data prep complete.\")\n",
    "\n",
    "# --- Step B: Model Training and Hyperparameter Tuning ---\n",
    "\n",
    "# 1. Define the base pipeline (Preprocessor -> Classifier)\n",
    "base_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    # Initial model with a placeholder random_state\n",
    "    ('classifier', XGBClassifier(use_label_encoder=False, \n",
    "                                    eval_metric='logloss',\n",
    "                                    random_state=42,\n",
    "                                    scale_pos_weight=4)\n",
    "    ) \n",
    "])\n",
    "\n",
    "# 2. Define the parameter grid for RandomizedSearchCV\n",
    "# Note: Parameters are prefixed with the estimator name, e.g., 'classifier__max_depth'\n",
    "# REVISED RANGES based on previous run's boundary hits:\n",
    "param_distributions = {\n",
    "    'classifier__n_estimators': randint(50, 500), # Unchanged\n",
    "    'classifier__learning_rate': uniform(0.01, 0.3), # Unchanged\n",
    "    'classifier__max_depth': randint(3, 15), # EXPANDED: from 3-10 to 3-15 (i.e., max depth up to 14)\n",
    "    'classifier__colsample_bytree': uniform(0.4, 0.6), # EXPANDED: from 0.6-1.0 to 0.4-1.0 (start lower)\n",
    "    'classifier__gamma': uniform(0, 1.0), # EXPANDED: from 0-0.5 to 0-1.0 (allow higher regularization)\n",
    "}\n",
    "\n",
    "# 3. Setup Cross-Validation\n",
    "# Use StratifiedKFold to maintain the proportion of the target variable (is_delayed) in each fold\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# 4. Initialize and run RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=base_pipeline,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=50, # Number of parameter settings that are sampled (adjust based on resources)\n",
    "    scoring='roc_auc', # Metric to optimize for (ROC AUC is good for imbalance)\n",
    "    cv=kf,\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1 # Use all available cores\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"--- STEP B: Starting Hyperparameter Tuning (Randomized Search) ---\")\n",
    "print(\"Optimization Metric: ROC AUC (Good for Imbalanced Data)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "print(\"\\nHyperparameter tuning complete.\")\n",
    "print(f\"Best ROC AUC Score found: {random_search.best_score_:.4f}\")\n",
    "print(\"\\nBest Parameters Found:\")\n",
    "# Print only the classifier parameters for clarity\n",
    "best_params_clean = {k.replace('classifier__', ''): v for k, v in random_search.best_params_.items()}\n",
    "for k, v in best_params_clean.items():\n",
    "    print(f\"  {k}: {v:.4f}\")\n",
    "    \n",
    "# 5. Evaluate the best model (Simple Accuracy)\n",
    "train_accuracy = best_model.score(X_train, y_train)\n",
    "test_accuracy = best_model.score(X_test, y_test)\n",
    "\n",
    "print(f\"\\nTraining Accuracy (Best Model): {train_accuracy:.4f}\")\n",
    "print(f\"Test Accuracy (Best Model): {test_accuracy:.4f}\")\n",
    "\n",
    "# --- Step C: Comprehensive Model Evaluation (using best_model) ---\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"--- STEP C: DETAILED MODEL EVALUATION (BEST MODEL ON TEST SET) ---\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 1. Generate predictions and probabilities for the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "# We need probabilities for the ROC curve (class 1: delayed)\n",
    "y_pred_proba = best_model.predict_proba(X_test)[:, 1] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24656cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2. Classification Report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Not Delayed (0)', 'Delayed (1)']))\n",
    "\n",
    "# 3. Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(7, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Predicted 0', 'Predicted 1'], \n",
    "            yticklabels=['Actual 0', 'Actual 1'])\n",
    "plt.title('Confusion Matrix (Best Model)')\n",
    "plt.xlabel('Prediction')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "print(\"Confusion Matrix plot displayed.\")\n",
    "\n",
    "# 4. ROC AUC Curve\n",
    "plt.figure(figsize=(8, 7))\n",
    "RocCurveDisplay.from_predictions(y_test, y_pred_proba, name='XGBoost (Tuned)')\n",
    "plt.plot([0, 1], [0, 1], 'r--', label='Random Guess') # Diagonal line\n",
    "plt.title('ROC Curve for Delay Prediction (Tuned Model)')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate (Recall)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "print(\"ROC AUC Curve plot displayed.\")\n",
    "\n",
    "roc_auc = auc(*roc_curve(y_test, y_pred_proba)[0:2])\n",
    "print(f\"\\nTest Set ROC AUC Score (Best Model): {roc_auc:.4f}\")\n",
    "\n",
    "print(\"\\nThe full, optimized pipeline is ready for prediction.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82553f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step D: Feature Importance Visualization ---\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"--- STEP D: FEATURE IMPORTANCE ANALYSIS ---\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 1. Get the trained classifier from the best pipeline\n",
    "final_model = best_model.named_steps['classifier']\n",
    "\n",
    "# 2. Extract feature names from the ColumnTransformer in the pipeline\n",
    "# The get_feature_names_out() method provides the final feature names\n",
    "# in the correct order: num, cat (OHE), bin (passthrough)\n",
    "feature_names = best_model.named_steps['preprocessor'].get_feature_names_out()\n",
    "\n",
    "# 3. Create a DataFrame for importance\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': final_model.feature_importances_\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# 4. Select top 20 features\n",
    "top_n = 20\n",
    "top_features = feature_importance_df.head(top_n).copy()\n",
    "\n",
    "# 5. Clean up feature names for better visualization \n",
    "# (Removes prefixes like 'cat__', 'num__', 'bin__')\n",
    "top_features['Feature'] = top_features['Feature'].str.replace(r'^(cat|num|bin)__', '', regex=True)\n",
    "\n",
    "# 6. Plotting\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(x='Importance', y='Feature', data=top_features, palette='viridis')\n",
    "plt.title(f'Top {top_n} XGBoost Feature Importances (Gain)')\n",
    "plt.xlabel('Feature Importance (F Score / Gain)')\n",
    "plt.ylabel('Feature')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(f\"Top {top_n} Feature Importance plot displayed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d138c307",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filename = \"best_xgb_pipeline.joblib\"\n",
    "try:\n",
    "    joblib.dump(best_model, model_filename)\n",
    "    print(f\"\\nModel successfully saved to {model_filename}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nWarning: Could not save model to file. Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5547788d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filename = \"best_xgb_pipeline.joblib\"\n",
    "try:\n",
    "    loaded_model = joblib.load(model_filename)\n",
    "    print(f\"Model successfully loaded from {model_filename}.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Could not load model. File '{model_filename}' not found. Make sure the file exists after training.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during model loading: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92eafc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_parquet('df_planned_test.parquet')\n",
    "try:\n",
    "    # 1. Load the model saved in Step B\n",
    "    # NOTE: Using 'loaded_model' from Step E if available, otherwise loading again.\n",
    "    if 'loaded_model' not in locals():\n",
    "        loaded_model = joblib.load(model_filename)\n",
    "        print(f\"Model loaded from {model_filename}.\")\n",
    "    \n",
    "    # 2. Loading the data\n",
    "    df_future = df_test\n",
    "    \n",
    "    # 3. Clean and transform columns to match the model's required input\n",
    "    df_to_predict = clean_and_rename_future_data(df_future.copy())\n",
    "    \n",
    "    # 4. Generate predictions (0: Not Delayed, 1: Delayed)\n",
    "    future_predictions = loaded_model.predict(df_to_predict)\n",
    "    \n",
    "    # 5. Get prediction probabilities\n",
    "    future_probas = loaded_model.predict_proba(df_to_predict)[:, 1]\n",
    "    \n",
    "    # 6. Store results\n",
    "    df_future['Predicted_Delay'] = future_predictions\n",
    "    df_future['Delay_Probability'] = future_probas\n",
    "\n",
    "    print(f\"\\nPredictions generated for {len(df_future)} future trips.\")\n",
    "    print(f\"Number of predicted delays (1): {df_future['Predicted_Delay'].sum()}\")\n",
    "    print(\"\\nFirst 10 predictions:\")\n",
    "    print(df_future[['AdvertisedTrainIdent', 'duration_minutes', 'distance_km', 'Predicted_Delay', 'Delay_Probability']].head(10))\n",
    "    \n",
    "    # Example of how to filter and inspect high-risk trips\n",
    "    high_risk_trips = df_future[df_future['Predicted_Delay'] == 1].sort_values(by='Delay_Probability', ascending=False)\n",
    "    if not high_risk_trips.empty:\n",
    "        print(f\"\\nTop 10 Highest-Risk Trips (Predicted Delayed):\")\n",
    "        print(high_risk_trips.head(10)[['AdvertisedTrainIdent', 'start_station', 'end_station', 'Delay_Probability']])\n",
    "    else:\n",
    "        print(\"\\nNo delays were predicted in the simulated future data sample.\")\n",
    "        \n",
    "except FileNotFoundError:\n",
    "    print(f\"\\nPrediction failed: Model file '{model_filename}' not found. Run Step B first to save the model.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred during future prediction: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c954a038",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
